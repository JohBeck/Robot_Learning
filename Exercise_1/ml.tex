\exercise{Machine Learning in a Nutshell}
For this exercise you will use a dataset, divided into training set and validation set (both attached). The first row is the vector $\vec x$ and the second row the vector $\vec y.$
\\
Based on these data, we want to learn a function mapping from $\vec x$ values to $\vec y$ values, of the form $\vec y=\vec{\theta}^{T}\vec{\phi}(\vec x)$.

For all questions requiring a plot, you also have to provide a snippet of your code!

You are allowed to use \texttt{scipy.spatial.distance.cdist} and \texttt{scipy.exp}.

\begin{questions}
	
	%----------------------------------------------
	
	\begin{question}{Supervised vs Unsupervised Learning}{2}
		Briefly explain the differences between supervised and unsupervised learning. 
		Is the above a supervised or unsupervised learning problem? Why?

\begin{answer}
	
	In supervised learning the algorithm learns from examples with solutions. Both input and output data are provided meaning the algorithm can learn by comparing new values with already learned data. Possible usecases could be clustering tasks.
	On the other side unsupervised learning is based on only one dataset without any further information. Here only the input data is provided and the algorithm has to learn all by himself. Based on this the above example is a supervised learning problem, due to the fact that the training set also provides the desired values for the given task. The algorithm is learning based on these existing results and not only from input data

\end{answer}
	\end{question}
	
	
	%----------------------------------------------
	
	\begin{question}{Regression vs Classification}{2}
		Supervised learning is typically divided into regression and classification tasks. 
		Briefly explain what are the differences between regression and classification.
		
\begin{answer}
Regression tasks predict a future state starting from the current situation and output continuous values.
Classification tasks cluster the data in groups and output them as class labels.
\end{answer}
\end{question}
	
	
	%----------------------------------------------
	
	\begin{question}{Linear Least Squares}{4}
		Consider the training set above to calculate features $\vec{\phi}(x)$ of the form $[\sin(2^{i}x)]_{i = 0 \ldots n-1}$. 
		Compute the feature values when $n$ is 2, 3 and 9 (i.e., when using 2, 3 and 9 features). 
		Use the linear least squares (LLS) method to predict output values $y$ for input values $x\in\{0, 0.01, 0.02, \ldots, 6\}$ using the different numbers of features. 
		Attach a single plot showing the three resulting predictions when using 2, 3 and 9 features (i.e., having $x$ and $y$ as axes).
		
\begin{answer}
%TODO
\end{answer}
		
	\end{question}
	
	
	%----------------------------------------------
	
	\begin{question}{Training a Model}{2}
		The root mean square error (RMSE) is defined as $\text{RMSE} = \sqrt{\frac{1}{N}\sum_{i=1}^{N}(y^{\text{true}}_i-y^{\text{predicted}}_i)^{2}}$, where $N$ is the number of data points. 
		Using the LLS algorithm implemented in the previous exercise, train a different model for each of the number of features between 1 and 9, i.e.,  [1,2,3...,9].
		For each of these models compute the corresponding RMSE for the training set. 
		Attach a plot where the x-axis represents the number of features and the y-axis represents the RMSE.
		
\begin{answer}
%TODO
\end{answer}
		
	\end{question}
	
	
	%----------------------------------------------
	
	\begin{question}{Model Selection}{4}
		Using the models trained in the previous exercise, compute the RMSE of each of these models for the validation set.\\
		Compare in one plot the RMSE on the training set and on the validation set. 
		How do they differ? 
		Can you explain what is the reason for these differences? (Hint: remember the plot from Exercise~c)~) 
		What is the number of features that you should use to achieve a proper modeling?
		
\begin{answer}
%TODO
\end{answer}
	\end{question}
	
	%----------------------------------------------
	
	\begin{question}{Cross Validation}{8}
		$K$-fold cross validation is a common approach to estimate the test error when the dataset is small.
		The idea is to randomly divide the training set into $K$ different datasets.
		Each of these datasets is then used as validation set for the model trained from the remaining $K-1$ datasets.
		The resulting vector of errors $\vec E = [ e_1... e_K ]$ can now be used to compute a distribution (typically by fitting a Gaussian distribution).
		When $K$ is equal to the number of data points, $K$-fold cross validation takes the name of leave-one-out cross validation (LOO).
		
		Apply LOO using only the training set and compute the mean/variance of the RMSE for the learned models. 
		Repeat for the models with the number of features between 1 and 9, i.e.,  [1,2,3...,9]
		
		Attach a plot showing the mean/variance (as a distribution) of the RMSE computed using LOO and having on the x-axis the number of features and on the y-axis the RMSE.
		Which is the optimal number of features now? 
		Discuss the results obtained and compare against model selection using train/validation set.
		
\begin{answer}
%TODO
\end{answer}
	\end{question}
	
	
	
	%----------------------------------------------
	
	
	\begin{question}{Kernel Functions}{2}
		A kernel function $k(\vec{\mbox{x}}_{i},\vec{\mbox{x}}_{j})$ is given by the inner product of two feature vectors. Write out the kernel function for the previous set of features where $n=3$.
		
\begin{answer}
%TODO
\end{answer}
	\end{question}
	
	
	%----------------------------------------------
	
	\begin{question}{Kernel Regression}{6}
		
		The kernel function in the previous question required explicit definition of the type and number of features, which is often difficult in practice. Instead, we can use a kernel that defines an inner product in a (possibly infinite dimensional) feature space. \\
		Using the training set and an exponential squared kernel 		$k( \vec{x}_{i} , \vec{x}_{j} )= \exp ( -\frac{1}{\sigma^{2}} \Vert \vec{x}_{i}-\vec{x}_{j}\Vert ^{2} )$ with $\sigma=0.15$, predict output values $y$ for input values $x\in\{0,0.01,\ldots,6\}$. Attach a plot of your results.
		\\
		(Hint: use standard kernel regression: $f(\mbox{\ensuremath{\vec{x}}})=\vec{k}\T\vec{K}^{-1}\vec{y}$ with $\vec{K}_{ij}=k(\vec{x}_{i},\vec{x}_{j})$ and $\vec{k}_{i}=k(\vec{x},\vec{x}_{i})$).
		
		Compute the RMSE on the validation set for the kernel regression model. Compare it with the RMSE of the best LLS model you found.
		
\begin{answer}
%TODO
\end{answer}
	\end{question}
	
	
	%----------------------------------------------
	
	\begin{question}[bonus]{Derivation}{5}
		Explain the concept of ridge regression and why/when it is used.
		Derive its final equations presented during the lecture.\\
		(Hint: remind that for normal linear regression the cost function is $J = \frac{1}{2}\sum_{i=1}^N ( f(\vec x_i) - y_i ) ^2 \,$ )\\
		(Hint 2: use matrix notation)
		
\begin{answer}
%TODO
\end{answer}
	\end{question}	
	
\end{questions}

